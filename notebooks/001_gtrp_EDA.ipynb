{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-auth in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (2.23.2)\n",
      "Requirement already satisfied: google-auth-oauthlib in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: google-auth-httplib2 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: google-cloud-bigquery in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (3.11.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-auth) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-auth) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-auth-oauthlib) (1.3.1)\n",
      "Requirement already satisfied: httplib2>=0.19.0 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-auth-httplib2) (0.22.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.47.0 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-cloud-bigquery) (1.59.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-cloud-bigquery) (2.12.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-cloud-bigquery) (1.22.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-cloud-bigquery) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-cloud-bigquery) (2.6.0)\n",
      "Requirement already satisfied: packaging>=20.0.0 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-cloud-bigquery) (23.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-cloud-bigquery) (4.24.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-cloud-bigquery) (2.31.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.60.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.59.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.5.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2023.7.22)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\cursos\\fiap_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > ..//requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _spark import *\n",
    "from transformations import transform\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = get_spark()\n",
    "\n",
    "gcs_bucket =  'tech-challenge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "    .read\\\n",
    "    .option('delimiter',',')\\\n",
    "    .option('header',True)\\\n",
    "    .option('inferSchema',True)\\\n",
    "    .csv('../data/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery links\n",
    "\n",
    "- [BigQuery Table](https://console.cloud.google.com/bigquery?hl=pt-br&project=fiap-tech-challenge-3&ws=!1m0)\n",
    "- [Storage](https://console.cloud.google.com/storage/browser/tech-challenge;tab=configuration?hl=pt-br&project=fiap-tech-challenge-3&prefix=&forceOnObjectsSortingFiltering=false)\n",
    "- [IAM e admin](https://console.cloud.google.com/iam-admin/iam?hl=pt-br&project=fiap-tech-challenge-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data\n",
    "\n",
    "#### Fato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a PySpark DataFrame to a BigQuery table\n",
    "\n",
    "df.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_f_covid_2020\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('uf', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_uf = spark.createDataFrame(data=[\n",
    "    {\n",
    "        \"11\": \"Rondônia\",\n",
    "        \"12\": \"Acre\",\n",
    "        \"13\": \"Amazonas\",\n",
    "        \"14\": \"Roraima\",\n",
    "        \"15\": \"Pará\",\n",
    "        \"16\": \"Amapá\",\n",
    "        \"17\": \"Tocantins\",\n",
    "        \"21\": \"Maranhão\",\n",
    "        \"22\": \"Piauí\",\n",
    "        \"23\": \"Ceará\",\n",
    "        \"24\": \"Rio Grande do Norte\",\n",
    "        \"25\": \"Paraíba\",\n",
    "        \"26\": \"Pernambuco\",\n",
    "        \"27\": \"Alagoas\",\n",
    "        \"28\": \"Sergipe\",\n",
    "        \"29\": \"Bahia\",\n",
    "        \"31\": \"Minas Gerais\",\n",
    "        \"32\": \"Espírito Santo\",\n",
    "        \"33\": \"Rio de Janeiro\",\n",
    "        \"35\": \"São Paulo\",\n",
    "        \"41\": \"Paraná\",\n",
    "        \"42\": \"Santa Catarina\",\n",
    "        \"43\": \"Rio Grande do Sul\",\n",
    "        \"50\": \"Mato Grosso do Sul\",\n",
    "        \"51\": \"Mato Grosso\",\n",
    "        \"52\": \"Goiás\",\n",
    "        \"53\": \"Distrito Federal\",\n",
    "    }], schema=_schema)\n",
    "\n",
    "_uf.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_d_uf\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('area_domicilio', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_area_domicilio = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Urbana',\n",
    "        '2': 'Rural',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_area_domicilio.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_d_area_domicilio\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('sexo', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_sexo = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Masculino',\n",
    "        '2': 'Feminino',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_sexo.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_d_sexo\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('raca', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_raca = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Branca',\n",
    "        '2': 'Preta',\n",
    "        '3': 'Amarela',\n",
    "        '4': 'Parda',\n",
    "        '5': 'Indígena',\n",
    "        '9': 'Ignorado',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_raca.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_d_raca\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('escolaridade', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_escolaridade = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Sem instrução',\n",
    "        '2': 'Fundamental incompleto',\n",
    "        '3': 'Fundamental completa',\n",
    "        '4': 'Médio incompleto',\n",
    "        '5': 'Médio completo',\n",
    "        '6': 'Superior incompleto',\n",
    "        '7': 'Superior completo',\n",
    "        '8': 'Pós-graduação, mestrado ou doutorado',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_escolaridade.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_d_escolaridade\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('escolaridade', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_resposta_covid = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Sim',\n",
    "        '2': 'Não ',\n",
    "        '3': 'Não sabe',\n",
    "        '9': 'Ignorado',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_resposta_covid.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_d_resposta_covid\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('resposta_internado', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_resposta_internado = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Sim',\n",
    "        '2': 'Não ',\n",
    "        '3': 'Não foi atendido',\n",
    "        '9': 'Ignorado',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_resposta_internado.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_d_resposta_internado\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('resposta_faixa_rendimento', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_resposta_faixa_rendimento = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '00':   '0 - 100',\n",
    "        '01':\t'101 - 300',\n",
    "        '02':\t'301 - 600',\n",
    "        '03':\t'601 - 800',\n",
    "        '04':\t'801 - 1.600',\n",
    "        '05':\t'1.601 - 3.000',\n",
    "        '06':\t'3.001 - 10.000',\n",
    "        '07':\t'10.001 - 50.000',\n",
    "        '08':\t'50.001 - 100.000',\n",
    "        '09':\t'Mais de 100.000',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_resposta_faixa_rendimento.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_d_resposta_faixa_rendimento\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('resposta_situacao_domicilio', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_resposta_situacao_domicilio = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Próprio - já pago ',\n",
    "        '2': 'Próprio - ainda pagando',\n",
    "        '3': 'Alugado',\n",
    "        '4': 'Cedido por empregador',\n",
    "        '5': 'Cedido por familiar ',\n",
    "        '6': 'Cedido de outra forma ',\n",
    "        '7': 'Outra condição',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_resposta_situacao_domicilio.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_d_resposta_situacao_domicilio\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('questao', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_mapa_questoes = spark.createDataFrame(data=[{  \n",
    "    \"UF\": \"uf\"\n",
    "  , \"V1012\": \"semana_mes\"\n",
    "  , \"V1013\": \"mes\"\n",
    "  , \"V1022\": \"area_domicilio\"\n",
    "  , \"A002\": \"idade\"\n",
    "  , \"A003\": \"sexo\"\n",
    "  , \"A004\": \"cor_raca\"\n",
    "  , \"A005\": \"escolaridade\"\n",
    "  , \"B0011\": \"teve_febre\"\n",
    "  , \"B0014\": \"teve_dificuldade_respirar\"\n",
    "  , \"B0015\": \"teve_dor_cabeca\"\n",
    "  , \"B0019\": \"teve_fadiga\"\n",
    "  , \"B00111\": \"teve_perda_cheiro\"\n",
    "  , \"B002\": \"foi_posto_saude\"\n",
    "  , \"B0031\": \"ficou_em_casa\"\n",
    "  , \"B005\": \"ficou_internado\"\n",
    "  , \"B007\": \"tem_plano_saude\"\n",
    "  , \"C007B\": \"assalariado\"\n",
    "  , \"C01011\": \"faixa_rendimento\"\n",
    "  , \"F001\": \"situacao_domicilio\"\n",
    "  }], schema=_schema)\n",
    "\n",
    "_mapa_questoes.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_d_questoes\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('sexo', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_sexo = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Masculino',\n",
    "        '2': 'Feminino',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_sexo.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'raw_pnad')\\\n",
    "    .option(\"table\", \"tb_d_sexo\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refined Data\n",
    "\n",
    "#### Fato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "    .read\\\n",
    "    .option('delimiter',',')\\\n",
    "    .option('header',True)\\\n",
    "    .option('inferSchema',True)\\\n",
    "    .csv('../data/raw')\n",
    "\n",
    "columns = [\n",
    "    \"UF\", \"V1012\", \"V1013\", \"V1022\", \"A002\", \"A003\",\n",
    "    \"A004\", \"A005\", \"B0011\", \"B0014\", \"B0015\", \"B0019\",\n",
    "    \"B00111\", \"B002\", \"B0031\", \"B005\", \"B007\", \"C007B\",\n",
    "    \"C01011\", \"F001\",'B009B'\n",
    "]\n",
    "\n",
    "df = df.select(columns)\n",
    "\n",
    "df = df\\\n",
    "        .withColumnRenamed(\"UF\", \"uf\")\\\n",
    "        .withColumnRenamed(\"V1012\", \"semana_mes\")\\\n",
    "        .withColumnRenamed(\"V1013\", \"mes\")\\\n",
    "        .withColumnRenamed(\"V1022\", \"area_domicilio\")\\\n",
    "        .withColumnRenamed(\"A002\", \"idade\")\\\n",
    "        .withColumnRenamed(\"A003\", \"sexo\")\\\n",
    "        .withColumnRenamed(\"A004\", \"cor_raca\")\\\n",
    "        .withColumnRenamed(\"A005\", \"escolaridade\")\\\n",
    "        .withColumnRenamed(\"B0011\", \"teve_febre\")\\\n",
    "        .withColumnRenamed(\"B0014\", \"teve_dificuldade_respirar\")\\\n",
    "        .withColumnRenamed(\"B0015\", \"teve_dor_cabeca\")\\\n",
    "        .withColumnRenamed(\"B0019\", \"teve_fadiga\")\\\n",
    "        .withColumnRenamed(\"B00111\", \"teve_perda_cheiro\")\\\n",
    "        .withColumnRenamed(\"B002\", \"foi_posto_saude\")\\\n",
    "        .withColumnRenamed(\"B0031\", \"ficou_em_casa\")\\\n",
    "        .withColumnRenamed(\"B005\", \"ficou_internado\")\\\n",
    "        .withColumnRenamed(\"B009B\", \"resultado_covid\")\\\n",
    "        .withColumnRenamed(\"B007\", \"tem_plano_saude\")\\\n",
    "        .withColumnRenamed(\"C007B\", \"assalariado\")\\\n",
    "        .withColumnRenamed(\"C01011\", \"faixa_rendimento\")\\\n",
    "        .withColumnRenamed(\"F001\", \"situacao_domicilio\")\n",
    "\n",
    "df.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_f_covid_2020\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('uf', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_uf = spark.createDataFrame(data=[\n",
    "    {\n",
    "        \"11\": \"Rondônia\",\n",
    "        \"12\": \"Acre\",\n",
    "        \"13\": \"Amazonas\",\n",
    "        \"14\": \"Roraima\",\n",
    "        \"15\": \"Pará\",\n",
    "        \"16\": \"Amapá\",\n",
    "        \"17\": \"Tocantins\",\n",
    "        \"21\": \"Maranhão\",\n",
    "        \"22\": \"Piauí\",\n",
    "        \"23\": \"Ceará\",\n",
    "        \"24\": \"Rio Grande do Norte\",\n",
    "        \"25\": \"Paraíba\",\n",
    "        \"26\": \"Pernambuco\",\n",
    "        \"27\": \"Alagoas\",\n",
    "        \"28\": \"Sergipe\",\n",
    "        \"29\": \"Bahia\",\n",
    "        \"31\": \"Minas Gerais\",\n",
    "        \"32\": \"Espírito Santo\",\n",
    "        \"33\": \"Rio de Janeiro\",\n",
    "        \"35\": \"São Paulo\",\n",
    "        \"41\": \"Paraná\",\n",
    "        \"42\": \"Santa Catarina\",\n",
    "        \"43\": \"Rio Grande do Sul\",\n",
    "        \"50\": \"Mato Grosso do Sul\",\n",
    "        \"51\": \"Mato Grosso\",\n",
    "        \"52\": \"Goiás\",\n",
    "        \"53\": \"Distrito Federal\",\n",
    "    }], schema=_schema)\n",
    "\n",
    "_uf.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_d_uf\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o711.save.\n: com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery\r\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:111)\r\n\tat com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)\r\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)\r\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 5.0 failed 1 times, most recent failure: Lost task 30.0 in stage 5.0 (TID 128) (DESKTOP-1B13685 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 30 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.immutable.List.foreach(List.scala:333)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:106)\r\n\t... 44 more\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 30 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge_3\\notebooks\\001_gtrp_EDA.ipynb Cell 24\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m _schema \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mStructType(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     [\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m           t\u001b[39m.\u001b[39mStructField(\u001b[39m'\u001b[39m\u001b[39mcd\u001b[39m\u001b[39m'\u001b[39m, t\u001b[39m.\u001b[39mStringType())\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         , t\u001b[39m.\u001b[39mStructField(\u001b[39m'\u001b[39m\u001b[39marea_domicilio\u001b[39m\u001b[39m'\u001b[39m, t\u001b[39m.\u001b[39mStringType())\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     ]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m _area_domicilio \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(data\u001b[39m=\u001b[39m[\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     {\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mUrbana\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mRural\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     }], schema\u001b[39m=\u001b[39m_schema)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m _area_domicilio\u001b[39m.\u001b[39;49mwrite\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mbigquery\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mtemporaryGcsBucket\u001b[39;49m\u001b[39m\"\u001b[39;49m, gcs_bucket)\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mcredentialsFile\u001b[39;49m\u001b[39m\"\u001b[39;49m,os\u001b[39m.\u001b[39;49menviron[\u001b[39m\"\u001b[39;49m\u001b[39mGOOGLE_APPLICATION_CREDENTIALS\u001b[39;49m\u001b[39m\"\u001b[39;49m])\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mproject\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfiap-tech-challenge-3\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mparentProject\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfiap-tech-challenge-3\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m'\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrefined_pnad\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mtable\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtb_d_area_domicilio\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Cursos/FIAP_p%C3%B3s/gp27_techchallenge_3/notebooks/001_gtrp_EDA.ipynb#X32sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m.\u001b[39;49msave()\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1396\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[0;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1396\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1398\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave(path)\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge_3\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o711.save.\n: com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery\r\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:111)\r\n\tat com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)\r\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)\r\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 5.0 failed 1 times, most recent failure: Lost task 30.0 in stage 5.0 (TID 128) (DESKTOP-1B13685 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 30 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.immutable.List.foreach(List.scala:333)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:106)\r\n\t... 44 more\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 30 more\r\n"
     ]
    }
   ],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('area_domicilio', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_area_domicilio = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Urbana',\n",
    "        '2': 'Rural',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_area_domicilio.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_d_area_domicilio\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('sexo', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_sexo = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Masculino',\n",
    "        '2': 'Feminino',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_sexo.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_d_sexo\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('raca', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_raca = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Branca',\n",
    "        '2': 'Preta',\n",
    "        '3': 'Amarela',\n",
    "        '4': 'Parda',\n",
    "        '5': 'Indígena',\n",
    "        '9': 'Ignorado',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_raca.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_d_raca\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('escolaridade', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_escolaridade = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Sem instrução',\n",
    "        '2': 'Fundamental incompleto',\n",
    "        '3': 'Fundamental completa',\n",
    "        '4': 'Médio incompleto',\n",
    "        '5': 'Médio completo',\n",
    "        '6': 'Superior incompleto',\n",
    "        '7': 'Superior completo',\n",
    "        '8': 'Pós-graduação, mestrado ou doutorado',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_escolaridade.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_d_escolaridade\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('escolaridade', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_resposta_covid = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Sim',\n",
    "        '2': 'Não ',\n",
    "        '3': 'Não sabe',\n",
    "        '9': 'Ignorado',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_resposta_covid.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_d_resposta_covid\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('resposta_internado', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_resposta_internado = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Sim',\n",
    "        '2': 'Não ',\n",
    "        '3': 'Não foi atendido',\n",
    "        '9': 'Ignorado',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_resposta_internado.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_d_resposta_internado\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('resposta_faixa_rendimento', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_resposta_faixa_rendimento = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '00':   '0 - 100',\n",
    "        '01':\t'101 - 300',\n",
    "        '02':\t'301 - 600',\n",
    "        '03':\t'601 - 800',\n",
    "        '04':\t'801 - 1.600',\n",
    "        '05':\t'1.601 - 3.000',\n",
    "        '06':\t'3.001 - 10.000',\n",
    "        '07':\t'10.001 - 50.000',\n",
    "        '08':\t'50.001 - 100.000',\n",
    "        '09':\t'Mais de 100.000',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_resposta_faixa_rendimento.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_d_resposta_faixa_rendimento\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('resposta_situacao_domicilio', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_resposta_situacao_domicilio = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Próprio - já pago ',\n",
    "        '2': 'Próprio - ainda pagando',\n",
    "        '3': 'Alugado',\n",
    "        '4': 'Cedido por empregador',\n",
    "        '5': 'Cedido por familiar ',\n",
    "        '6': 'Cedido de outra forma ',\n",
    "        '7': 'Outra condição',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_resposta_situacao_domicilio.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_d_resposta_situacao_domicilio\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('questao', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_mapa_questoes = spark.createDataFrame(data=[{  \n",
    "    \"UF\": \"uf\"\n",
    "  , \"V1012\": \"semana_mes\"\n",
    "  , \"V1013\": \"mes\"\n",
    "  , \"V1022\": \"area_domicilio\"\n",
    "  , \"A002\": \"idade\"\n",
    "  , \"A003\": \"sexo\"\n",
    "  , \"A004\": \"cor_raca\"\n",
    "  , \"A005\": \"escolaridade\"\n",
    "  , \"B0011\": \"teve_febre\"\n",
    "  , \"B0014\": \"teve_dificuldade_respirar\"\n",
    "  , \"B0015\": \"teve_dor_cabeca\"\n",
    "  , \"B0019\": \"teve_fadiga\"\n",
    "  , \"B00111\": \"teve_perda_cheiro\"\n",
    "  , \"B002\": \"foi_posto_saude\"\n",
    "  , \"B0031\": \"ficou_em_casa\"\n",
    "  , \"B005\": \"ficou_internado\"\n",
    "  , \"B007\": \"tem_plano_saude\"\n",
    "  , \"C007B\": \"assalariado\"\n",
    "  , \"C01011\": \"faixa_rendimento\"\n",
    "  , \"F001\": \"situacao_domicilio\"\n",
    "  }], schema=_schema)\n",
    "\n",
    "_mapa_questoes.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_d_questoes\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = t.StructType(\n",
    "    [\n",
    "          t.StructField('cd', t.StringType())\n",
    "        , t.StructField('sexo', t.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "_sexo = spark.createDataFrame(data=[\n",
    "    {\n",
    "        '1': 'Masculino',\n",
    "        '2': 'Feminino',\n",
    "    }], schema=_schema)\n",
    "\n",
    "_sexo.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'refined_pnad')\\\n",
    "    .option(\"table\", \"tb_d_sexo\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trusted Data\n",
    "\n",
    "#### Fato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformations import transform\n",
    "from _spark import get_spark, _display\n",
    "import os\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = get_spark()\n",
    "gcs_bucket =  'tech-challenge'\n",
    "\n",
    "df = spark\\\n",
    "    .read\\\n",
    "    .option('delimiter',',')\\\n",
    "    .option('header',True)\\\n",
    "    .option('inferSchema',True)\\\n",
    "    .csv('../data/raw')\n",
    "\n",
    "df = transform(df)\n",
    "\n",
    "\n",
    "df.write\\\n",
    "    .format(\"bigquery\")\\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket)\\\n",
    "    .option(\"credentialsFile\",os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\\\n",
    "    .option(\"project\", \"fiap-tech-challenge-3\")\\\n",
    "    .option(\"parentProject\", \"fiap-tech-challenge-3\")\\\n",
    "    .option('dataset', 'trusted_pnad')\\\n",
    "    .option(\"table\", \"tb_f_covid_2020\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resultado_covid</th>\n",
       "      <th>count(uf)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA</td>\n",
       "      <td>737590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Não</td>\n",
       "      <td>314989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sim</td>\n",
       "      <td>95811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Não sabe</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ignorado</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  resultado_covid  count(uf)\n",
       "0              NA     737590\n",
       "1             Não     314989\n",
       "2             Sim      95811\n",
       "3        Não sabe        174\n",
       "4        Ignorado        633"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('resultado_covid').agg(f.count('uf')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
